# Defination:
```
In machine learning, especially in natural language processing (NLP),
tokenization is the process of breaking text into smaller units called tokens so the computer can work with them more easily.
```

What is Tokenization?

A token can be:
A word ("Machine", "learning")
A subword ("learn", "ing")
A character ("M", "a", "c")
Tokenization turns text into a sequence of tokens so that ML models can map them to numbers (embeddings).

Example:
```
Sentence: "Machine learning is fun!"
Tokens: ["Machine", "learning", "is", "fun", "!"]
```
